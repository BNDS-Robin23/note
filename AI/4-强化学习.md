# 强化学习
## 马尔可夫过程
+ 每个节点的当前状态只和上一个状态和状态转移矩阵有关

## 马尔可夫奖励过程
定义效用函数 两种定义
$V(s) = \sum_{s'}P(s'|s)(V(s')+R(s'))$
$V(s) = \sum_{s'}P(s'|s)(R(s')+\gamma V(s'))$
只和下一个状态有关
末状态的V为0，反向传播

## 马尔可夫决策过程
输入为$<S, A, R, P>$
策略$\pi$实际上为给定状态下各个动作的概率
修改效用函数的计算方法，实际上要对两个东西求和：一个是动作，一个是采取动作能够转移到的状态
![](img/2020-01-04-14-58-45.png)

## 定义Q值函数
实际上就是在当前节点的某个动作a下的效用函数
$Q^\pi(S, a) = \sum_{S'}P(S'|S, a)(V^\pi(S')+R(S, a, S'))$
![](img/2020-01-04-15-02-07.png)

## 最优解
![](img/2020-01-04-15-07-07.png)
也就是说，这是一个固定点，可以被不断强化逼近

## 找到马尔可夫决策过程中的最优解
### 策略评估
策略评估就是V和Q的反向更新公式
### 策略改进

![](img/2020-01-04-15-24-22.png)

#### 价值迭代
![](img/2020-01-04-15-32-35.png)
#### 策略迭代
![](img/2020-01-04-15-32-53.png)




## 在未知R和P的情况下学习
### 蒙特卡洛算法
![](img/2020-01-04-15-38-13.png)
每次更新的是一条蒙特卡洛采样路径上的点和对应动作
![](img/2020-01-04-15-39-09.png)
![](img/2020-01-04-15-39-27.png)

![](img/2020-01-04-15-43-06.png)
